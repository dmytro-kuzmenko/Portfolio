{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A 30-fps detection + tracking pipeline\n",
    "* @author Dmytro Kuzmenko\n",
    "* 1 detection / second\n",
    "* 29 tracking / second\n",
    "* Exit zones tracking, package ID and status tracking\n",
    "* Extra 'Receipt detection' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from scipy import spatial\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: normalized_input_image_tensor\n",
      "shape: [  1 512 512   3]\n",
      "type: <class 'numpy.uint8'>\n",
      "\n",
      "== Output details ==\n",
      "name: TFLite_Detection_PostProcess\n",
      "shape: [ 1 40  4]\n",
      "type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# Path to .tflite file of the model (detector)\n",
    "model_path = './model-export_model_5.tflite' \n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "BAG_DETECTION_SCORE_THRESHOLD = 0.3\n",
    "MIN_AREA_PROPORTION = 4\n",
    "MAX_AREA_PROPORTION = 27\n",
    "BOUNDING_RECT_2_CONTOUR_RATIO = 2.5\n",
    "ROTATED_RECT_2_APPROX_POLY_RATIO = 2\n",
    "INTERSECTION_PERCENT = 0.5\n",
    "RECEIPT_PADDING_X = 0\n",
    "RECEIPT_PADDING_Y = 0\n",
    "COLOR_FILTER_VALUE = 90\n",
    "TEXT_PATTERN = '#\\d\\d\\d*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions to process bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_res_into_opposite_points(box, height, width):\n",
    "    \n",
    "    xmin = int(max(1, (box[1] * width)))\n",
    "    ymin = int(max(1, (box[0] * height)))\n",
    "    \n",
    "    xmax = int(min(width, (box[3] * width)))\n",
    "    ymax = int(min(height, (box[2] * height)))\n",
    "    \n",
    "    return xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_box_into_coords(box):\n",
    "    \n",
    "    x1, y1, box_width, box_height = box[0], box[1], box[2], box[3]\n",
    "    \n",
    "    point_1 = (x1, y1)\n",
    "    point_2 = (x1, y1 - box_height)\n",
    "    point_3 = (x1 + box_width, y1 - box_height)\n",
    "    point_4 = (x1 + box_width, y1)\n",
    "    \n",
    "    return [point_1, point_2, point_3, point_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_opposite_points_into_normal(box):\n",
    "    \n",
    "    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
    "    \n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "    \n",
    "    return [x1, y1, box_width, box_height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_res_into_bbox(box, height, width):\n",
    "    \n",
    "    xmin = int(max(1, (box[1] * width)))\n",
    "    ymin = int(max(1, (box[0] * height)))\n",
    "    \n",
    "    xmax = int(min(width, (box[3] * width)))\n",
    "    ymax = int(min(height, (box[2] * height)))\n",
    "\n",
    "    box_width = xmax - xmin\n",
    "    box_height = ymax - ymin\n",
    "    \n",
    "    return xmin, ymin, box_width, box_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box draw functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(ID, bbox, current_frame, status):\n",
    "    \n",
    "    x, y, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "    label = str(ID) + ' - ' + status\n",
    "    cv2.putText(current_frame, label, (x + 15, y + 30), cv2.FONT_HERSHEY_PLAIN, 1.7, (255, 255, 255), 2)\n",
    "    cv2.rectangle(current_frame, (x, y), (x + w, y + h), (10, 220, 0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_temporary_boxes(ID, bbox, current_frame, status):\n",
    "    \n",
    "    x, y, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "    label = str(ID) + ' - ' + status\n",
    "    cv2.putText(current_frame, label, (x + 15, y + 30), cv2.FONT_HERSHEY_PLAIN, 1.7, (255, 255, 255), 2)\n",
    "    cv2.rectangle(current_frame, (x, y), (x + w, y + h), (0, 10, 220), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tracked_boxes(ID, bbox, current_frame, draw_temporary_boxes):\n",
    "    \n",
    "    x, y, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "    label = str(ID) + ' - ' + status\n",
    "    cv2.putText(current_frame, label, (x + 15, y + 30), cv2.FONT_HERSHEY_PLAIN, 1.7, (255, 255, 255), 2)\n",
    "    cv2.rectangle(current_frame, (x, y), (x + w, y + h), (220, 10, 0), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_box(height, width, box, img):\n",
    "    \n",
    "    # starting coordinates of the box\n",
    "    ymin = int(max(1, (box[0] * height)))\n",
    "    xmin = int(max(1, (box[1] * width)))\n",
    "    \n",
    "    # last coordinates of the box\n",
    "    ymax = int(min(height, (box[2] * height)))\n",
    "    xmax = int(min(width, (box[3] * width)))\n",
    "    \n",
    "    return img[ymin:ymax, xmin:xmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, size=600):\n",
    "    \n",
    "    width = int(size * image.shape[1] / image.shape[0] if image.shape[0] > image.shape[1] else size)\n",
    "    height = int(size * image.shape[0] / image.shape[1] if image.shape[0] < image.shape[1] else size)\n",
    "    \n",
    "    return cv2.resize(image, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(image, degree):\n",
    "    h, w = image.shape[:2]\n",
    "    cX, cY = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D((cX, cY), degree, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h))\n",
    "\n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_intersect(original_image, contour1, contour2):\n",
    "    blank = np.zeros(original_image.shape[0:2], dtype=np.uint8)\n",
    "    image1 = cv2.fillPoly(blank.copy(), pts =[contour1], color=255)\n",
    "    image2 = cv2.fillPoly(blank.copy(), pts =[contour2], color=255)\n",
    "\n",
    "    union = cv2.bitwise_or(image1, image2)\n",
    "    union_contours = cv2.findContours(union, cv2.RETR_LIST,  cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    area1 = cv2.contourArea(contour1)\n",
    "    area2 = cv2.contourArea(contour2)\n",
    "    union_area = sum([cv2.contourArea(contour) for contour in union_contours])\n",
    "    intersection_area = area1 + area2 - union_area\n",
    "\n",
    "    if intersection_area and min([area1, area2])/intersection_area >= INTERSECTION_PERCENT:\n",
    "        final_contour = max(union_contours, key=lambda contour: cv2.contourArea(contour))\n",
    "        return [final_contour]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicate_contours(contours, img):\n",
    "    while True:\n",
    "        n = len(contours)\n",
    "        again = False\n",
    "        for i in range(len(contours)):\n",
    "            for j in range(i):\n",
    "                intersect = contour_intersect(img, contours[i], contours[j])\n",
    "                if intersect:\n",
    "                    del contours[i]\n",
    "                    del contours[j]\n",
    "                    contours.append(intersect[0])\n",
    "                    again = True\n",
    "                    break\n",
    "            if again:\n",
    "                break\n",
    "        if len(contours) == n:\n",
    "            return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_contour(image, contour):\n",
    "    hi, wi= image.shape[:2]\n",
    "    (x, y, w, h) = cv2.boundingRect(contour)\n",
    "  \n",
    "    padding_x = RECEIPT_PADDING_X\n",
    "    padding_y = RECEIPT_PADDING_Y\n",
    "  \n",
    "    cropped = image[max(y - padding_y, 0)+2: min(y + h + padding_y, hi)-2, max(x - padding_x, 0)+2 : min(x + w + padding_x, wi)-2]\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_test(img, contour, show = False):\n",
    "    img_area = img.shape[0]*img.shape[1]\n",
    "\n",
    "    contour_area = cv2.contourArea(contour)\n",
    "    (x, y, w, h) = cv2.boundingRect(contour)\n",
    "    bounding_rect_area = w*h\n",
    "\n",
    "    if show:\n",
    "        print(int(img_area/contour_area), int(bounding_rect_area/contour_area))\n",
    "        cv2.drawContours(img, [contour], -1, (160, 160, 0), 3)\n",
    "    \n",
    "    if img_area/contour_area >= MIN_AREA_PROPORTION and img_area/contour_area <= MAX_AREA_PROPORTION and bounding_rect_area/contour_area <= BOUNDING_RECT_2_CONTOUR_RATIO:\n",
    "        rotated_rect = np.int0(cv2.boxPoints(cv2.minAreaRect(contour)))\n",
    "        rotated_rect_area = cv2.contourArea(rotated_rect)\n",
    "\n",
    "        poly = []\n",
    "        for epsilon in range(10, 155, 5):\n",
    "            poly0 = cv2.approxPolyDP(contour, epsilon=epsilon, closed=True)\n",
    "            if len(poly0) == 4:\n",
    "                poly = poly0\n",
    "            if len(poly0) <= 4:\n",
    "                break\n",
    "\n",
    "        if show:\n",
    "            cv2.drawContours(img, [contour], -1, (225, 30, 30), 3)\n",
    "            cv2.drawContours(img, [rotated_rect], -1, (30, 255, 30), 3)\n",
    "            print(img_area/bounding_rect_area, img_area/contour_area)\n",
    "            print('cnt', contour_area, 'bound_rect', bounding_rect_area, 'rot_rect', rotated_rect_area, end=', ')\n",
    "      \n",
    "        if poly != []:\n",
    "            poly_area = cv2.contourArea(poly)\n",
    "            if show:\n",
    "                print('poly', poly_area)\n",
    "                cv2.drawContours(img, [poly], -1, (30, 30, 225), 3)\n",
    "            if rotated_rect_area/poly_area <= ROTATED_RECT_2_APPROX_POLY_RATIO:\n",
    "                if show:\n",
    "                    print(int(rotated_rect_area/poly_area), True)\n",
    "                    cv2.imshow(img)\n",
    "                return True\n",
    "            \n",
    "    if show:\n",
    "        cv2.imshow(img)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_test(img, contour):\n",
    "    mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "    cv2.drawContours(mask, [contour], -1, 255, -1)\n",
    "    mask = cv2.erode(mask, None, iterations=2)\n",
    "    mean = cv2.mean(img, mask=mask)[:3]\n",
    "    contour_area = cv2.contourArea(contour)\n",
    "    \n",
    "    return all(map(lambda x: x >= COLOR_FILTER_VALUE, mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(region):\n",
    "    for i in range(-25, 25):\n",
    "        receipt = region.copy()\n",
    "        receipt = rotate(receipt, i)\n",
    "        extractedInformation = pytesseract.image_to_string(receipt)\n",
    "        extractedInformation = extractedInformation.replace(' ', '')\n",
    "        extractedInformation = extractedInformation.replace('\\n', '')\n",
    "        extractedInformation = extractedInformation.replace('\\x0c', '')\n",
    "        if extractedInformation:\n",
    "            all = re.findall(TEXT_PATTERN, extractedInformation)\n",
    "            if all:\n",
    "                print(i, ': ', all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(img):\n",
    "    img = img.copy() \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \n",
    "    \n",
    "    kernel = np.ones((1, 1), np.uint8)    \n",
    "    img = cv2.dilate(img, kernel, iterations=1)    \n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bags(frame, update_dict, active_objects, show = False):\n",
    "    \n",
    "    img_raw = resize_image(frame, 900)\n",
    "  \n",
    "    if show:\n",
    "        img_boxes_show = frame.copy() \n",
    "    \n",
    "    img_input = cv2.resize(img_raw, (512, 512))\n",
    "    img_input = np.reshape(img_input, (1, 512, 512,3))\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], img_input)\n",
    "    interpreter.invoke()\n",
    "    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "    scores = output_data = interpreter.get_tensor(output_details[2]['index'])[0]\n",
    "      \n",
    "    imH, imW, _ = frame.shape\n",
    "    \n",
    "    final_boxes = []\n",
    "    final_box_points = [] \n",
    "    \n",
    "    ID = 0\n",
    "\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] > BAG_DETECTION_SCORE_THRESHOLD:\n",
    "            ID += 1\n",
    "            \n",
    "            local_box = boxes[i]\n",
    "            bbox_tuple = tuple(local_box)\n",
    "            x, y, ww, hh = convert_res_into_bbox(bbox_tuple, imH, imW)\n",
    "            \n",
    "            x1, y1, x2, y2 = convert_res_into_opposite_points(bbox_tuple, imH, imW)\n",
    "            \n",
    "            boxes_points = [x1, y1, x2, y2]\n",
    "\n",
    "            boxes_coords = [x, y, ww, hh]\n",
    "            final_boxes.append(boxes_coords)\n",
    "            final_box_points.append(boxes_points)\n",
    "            \n",
    "            if update_dict:\n",
    "                is_constant = False\n",
    "                status = 'is_preparing'\n",
    "                package_id_dict.update({ID: (boxes_coords, 0, is_constant, status)})\n",
    "                global objects_registered \n",
    "                objects_registered += 1\n",
    "                active_objects += 1\n",
    "                print(f'INSIDE. Now tracking new package with ID {ID}, coordinates: {boxes_coords}')\n",
    "            \n",
    "            if show:\n",
    "                print(\"The box {} has probability {}\".format(boxes_coords, scores[i]))\n",
    "    \n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(img_boxes_show, cv2.COLOR_BGR2RGB))\n",
    "  \n",
    "    return final_boxes, active_objects, final_box_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZoneLabeler class to label initial exit zones on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZoneLabeler:\n",
    "    \n",
    "    def __init__(self, img):\n",
    "        \n",
    "        self.polygons = []\n",
    "        self._window_height = 720\n",
    "        self._window_width = 1280\n",
    "        self._img = img\n",
    "        self._height_ratio = self._img.shape[0] / self._window_height\n",
    "        self._width_ratio = self._img.shape[1] / self._window_width\n",
    "        if self._height_ratio > 1 or self._width_ratio > 1:\n",
    "            interpol_method = cv2.INTER_AREA\n",
    "        else:\n",
    "            interpol_method = cv2.INTER_LINEAR\n",
    "        self._processed_img = cv2.resize(img, (self._window_width, self._window_height), interpolation=interpol_method)\n",
    "        self._current_polygon = []\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        cv2.imshow(\"image\", self._processed_img)\n",
    "        cv2.setMouseCallback('image', self.on_click)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        return self._img, self.polygons\n",
    "\n",
    "    def on_click(self, event, x, y, p1, p2):\n",
    "        \n",
    "        custom_color = (130, 0, 75) # purple\n",
    "        \n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            cv2.circle(self._processed_img, (x, y), 3, custom_color, -1)\n",
    "            x = int(x * self._width_ratio)\n",
    "            y = int(y * self._height_ratio)\n",
    "            cv2.circle(self._img, (x, y), 3, custom_color, -1)\n",
    "\n",
    "            self._current_polygon.append([x, y])\n",
    "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "            self.polygons.append(self._current_polygon)\n",
    "            self._current_polygon = np.array([self._current_polygon])\n",
    "            cv2.polylines(self._img, [self._current_polygon], True, custom_color, thickness=3)\n",
    "            polygons_for_proc = [np.array([int(p[0] / self._width_ratio), int(p[1] / self._height_ratio)])\n",
    "                                 for p in self._current_polygon[0]]\n",
    "            polygons_for_proc = np.array([polygons_for_proc])\n",
    "            cv2.polylines(self._processed_img, [polygons_for_proc], True, custom_color, thickness=3)\n",
    "            self._current_polygon = []\n",
    "        cv2.imshow(\"image\", self._processed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exit zones handling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_exit_zones(package_id_dict, exit_zone_list):\n",
    "    \n",
    "    new_package_id_dict = package_id_dict.copy()\n",
    "    for ID, (box, frames_missing_since_last_detection, is_constant, status) in package_id_dict.items():\n",
    "        box_coords = convert_box_into_coords(box)\n",
    "        for exit_zone in exit_zone_list:\n",
    "            exit_zone_tupled = [(i[0], i[1]) for i in exit_zone]\n",
    "            \n",
    "            # create polys\n",
    "            box_poly = Polygon(box_coords)\n",
    "            exit_poly = Polygon(exit_zone_tupled)\n",
    "            \n",
    "            # get intersection / area of box\n",
    "            intersect_area = box_poly.intersection(exit_poly).area / box_poly.area\n",
    "            \n",
    "            # check threshold # >= 0.5\n",
    "            if intersect_area >= 0.5:\n",
    "                print('Package entered exit zone')\n",
    "                new_package_id_dict.update({ID: (box, frames_missing_since_last_detection, is_constant, 'in_exit_zone')})\n",
    "                \n",
    "    return new_package_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_exit_zones(img, exit_zone_list):\n",
    "    \n",
    "    custom_color = (130, 0, 75) # purple\n",
    "    for i, zone_list in enumerate(exit_zone_list):\n",
    "        polygons_for_proc = np.array(zone_list)\n",
    "        cv2.polylines(img, [polygons_for_proc], True, custom_color, thickness=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    pick = []\n",
    "\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    \n",
    "    \"\"\"\n",
    "        compute the area of the bounding boxes and sort the bounding\n",
    "        boxes by the bottom-right y-coordinate of the bounding box\n",
    "    \"\"\"\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    \n",
    "    # keep looping while some indexes still remain in the indexes list\n",
    "    while len(idxs) > 0:\n",
    "        \n",
    "        \"\"\"\n",
    "            grab the last index in the indexes list and add the\n",
    "            index value to the list of picked indexes\n",
    "        \"\"\"\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        \n",
    "        \"\"\"\n",
    "            find the largest (x, y) coordinates for the start of\n",
    "            the bounding box and the smallest (x, y) coordinates\n",
    "            for the end of the bounding box\n",
    "        \"\"\"\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        \n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "        \n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "        \n",
    "    return boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reidentification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions for status table rendering on frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status_table(status_table_dict, package_id_dict):\n",
    "    for ID, (_, _, _, status) in package_id_dict.items():\n",
    "        status_table_dict.update({ID: status})\n",
    "    return status_table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_status_table(current_frame, status_table_dict):\n",
    "    \n",
    "    y_expand = 0\n",
    "    rect_done = False\n",
    "    \n",
    "    # set max rect width\n",
    "    max_text_size = cv2.getTextSize('1 - is_preparing', cv2.FONT_HERSHEY_PLAIN, 1.7, 2)\n",
    "    max_text_width = max_text_size[0][0]\n",
    "    \n",
    "    obj_cnt = len(list(status_table_dict.keys()))\n",
    "    \n",
    "    for ID, status in status_table_dict.items():\n",
    "        label = str(ID) + ' - ' + status\n",
    "       \n",
    "        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1.7, 2)\n",
    "        text_w, text_h = text_size[0][0], text_size[1]\n",
    "        \n",
    "        y_expand += 30\n",
    "        \n",
    "        if not rect_done:\n",
    "            cv2.rectangle(current_frame, (0, 0), (0 + max_text_width, 0 + obj_cnt*(y_expand + text_h)), (0, 0, 0), -1)\n",
    "            rect_done = True\n",
    "        cv2.putText(current_frame, label, (0, 0 + y_expand), cv2.FONT_HERSHEY_PLAIN, 1.7, (255, 255, 255), 2)\n",
    "        y_expand += text_h "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## receipt detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proposed_regions(box, use_receipt_detection=False):\n",
    "\n",
    "    img = prepare_image(box)\n",
    "\n",
    "    proposed_contours = []\n",
    "    img_area = img.shape[0]*img.shape[1]\n",
    "\n",
    "    for threshold_value in range(60, 260, 20):\n",
    "        thresh0 = cv2.threshold(img, threshold_value, 255, cv2.THRESH_BINARY)[1]\n",
    "        thresh = cv2.adaptiveThreshold(thresh0, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                     cv2.THRESH_BINARY, 11, 2)\n",
    "  \n",
    "        contours = cv2.findContours(thresh.copy(), cv2.RETR_LIST,  cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "        contours = [contour for contour in contours if cv2.contourArea(contour) > 0]\n",
    "        contours = sorted(contours, key=lambda contour: cv2.contourArea(contour), reverse = True)\n",
    "        \n",
    "        colors = [(255, 30, 30), (30, 255, 30), (30, 30, 255)]\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                if shape_test(box.copy(), contours[i]):\n",
    "                    proposed_contours.append(contours[i])\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    filtered_contours = filter_duplicate_contours(proposed_contours, img)\n",
    "    filtered_contours = [contour for contour in filtered_contours if (img_area/cv2.contourArea(contour) >= MIN_AREA_PROPORTION and img_area/cv2.contourArea(contour) <= MAX_AREA_PROPORTION)]\n",
    "    filtered_contours = [contour for contour in filtered_contours if color_test(box, contour)]\n",
    "\n",
    "    output = box.copy()\n",
    "    for i in range(len(filtered_contours)):\n",
    "        cv2.drawContours(output, [filtered_contours[i]], -1, (255-50*i, 50*i, 50), 3)\n",
    "    if show:\n",
    "        cv2_imshow(output)\n",
    "  \n",
    "    proposed_regions = [crop_contour(img, contour) for contour in filtered_contours]\n",
    "  \n",
    "    return proposed_regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_proper_box(current_frame, package_id_dict):\n",
    "    \n",
    "    bags_in_img = [] \n",
    "    for ID, (box, _, _, _) in package_id_dict.items():\n",
    "        x, y, ww, hh = box[0], box[1], box[2], box[2]\n",
    "        \n",
    "        xmin, xmax = x, x+ww\n",
    "        ymin, ymax = y, y+hh\n",
    "        \n",
    "        bags_in_img.append(current_frame[int(ymin):int(ymax), int(xmin):int(xmax)])\n",
    "        \n",
    "    return bags_in_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_receipt_regions(bags_in_img, use_receipt_detection=False):\n",
    "    if not use_receipt_detection:\n",
    "        print(\"not detecting receipts in this run\")\n",
    "        return\n",
    "    \n",
    "    predicted_bags = bags_in_img\n",
    "    for bag_box in predicted_bags:\n",
    "        proposed_receipt_regions = get_proposed_regions(bag_box, use_receipt_detection)\n",
    "        for region in proposed_receipt_regions:\n",
    "            cv2.imshow(region)\n",
    "            read_text(region)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean distance\n",
    "def is_highly_similar(centroid1, centroid2):\n",
    "    \n",
    "    distance = spatial.distance.euclidean(centroid1, centroid2)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc centroid of a box\n",
    "def get_centroid(box):\n",
    "    \n",
    "    x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "    coord = (x, y, w, h)\n",
    "    center_coord = (coord[0] + (coord[2] / 2), coord[1] + (coord[3] / 2))\n",
    "    \n",
    "    return center_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video description from first frame\n",
    "def get_capture_info(cap):\n",
    "    \n",
    "    \"\"\"\n",
    "        cap - capture object instance\n",
    "        \n",
    "        returns - height of the frame, width of the frame, FPS of the capture, k - every k-th frame to process\n",
    "    \"\"\"\n",
    "    \n",
    "    success, first_frame = cap.read()\n",
    "    \n",
    "    if not success:\n",
    "        return 'Error: issues with video'\n",
    "\n",
    "    h, w = first_frame.shape[:2]\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # define k, which denotes every k-th frame to process - FPS / FRAMES_PROCESSED\n",
    "    k = fps / FRAMES_PROCESSED\n",
    "    \n",
    "    return h, w, fps, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reind method for cases: new_boxes_cnt == old_boxes_cnt, new_boxes_cnt > old_boxes_cnt\n",
    "def reindentify_boxes_v1(remaining_boxes, old_dict, new_dict):\n",
    "    used_old_boxes = []\n",
    "    \n",
    "    new_boxes_list = remaining_boxes\n",
    "    unused_new_boxes = new_boxes_list.copy()\n",
    "    \n",
    "    for new_box in new_boxes_list:\n",
    "\n",
    "        used_old_boxes = [list(i) for i in used_old_boxes]\n",
    "\n",
    "        similarity_dict = {}\n",
    "        for i, (ID, (old_box, frames_missing, is_constant, status)) in enumerate(old_dict.items()):\n",
    "\n",
    "            old_box = list(old_box)\n",
    "\n",
    "            if old_box in used_old_boxes:\n",
    "                continue\n",
    "            centroid_old = get_centroid(old_box)\n",
    "            centroid_new = get_centroid(new_box)\n",
    "            highly_similar = is_highly_similar(centroid_old, centroid_new)\n",
    "            similarity_dict.update({(ID, frames_missing, is_constant, status): (highly_similar, old_box)})        \n",
    "\n",
    "        if not similarity_dict:\n",
    "            continue\n",
    "            \n",
    "        most_similar_old_box = min(similarity_dict, key=similarity_dict.get)\n",
    "        used_old_boxes.append(similarity_dict[most_similar_old_box][1])\n",
    "        \n",
    "        needed_box = similarity_dict[most_similar_old_box]\n",
    "        \n",
    "        ID, frames_missing, is_constant, status = most_similar_old_box[0], most_similar_old_box[1], most_similar_old_box[2], most_similar_old_box[3]\n",
    "        new_dict.update({ID: (new_box, frames_missing, is_constant, status)})\n",
    "        unused_new_boxes.remove(new_box)\n",
    "            \n",
    "    \n",
    "    return new_dict, unused_new_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reind method for cases: new_boxes_cnt == old_boxes_cnt, new_boxes_cnt < old_boxes_cnt\n",
    "def reindentify_boxes_v2(remaining_boxes, old_dict, new_dict):\n",
    "    \n",
    "    used_new_box_ids = []\n",
    "    non_reidentified_box_ids = list(old_dict.keys())\n",
    "    \n",
    "    new_boxes_list = remaining_boxes\n",
    "    \n",
    "    for i, (ID, (old_box, frames_missing, is_constant, status)) in enumerate(old_dict.items()):\n",
    "        for new_box in new_boxes_list:\n",
    "            if ID in used_new_box_ids:\n",
    "                break\n",
    "            centroid_old = get_centroid(old_box)\n",
    "            centroid_new = get_centroid(new_box) \n",
    "            distance = is_highly_similar(centroid_old, centroid_new)\n",
    "            \n",
    "            if distance <= 30:\n",
    "                new_dict.update({ID: (new_box, frames_missing, is_constant, status)})\n",
    "                non_reidentified_box_ids.remove(ID)      \n",
    "\n",
    "    return new_dict, non_reidentified_box_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_detector(frame, reindentify, objects_deregistered, \n",
    "                 \\objects_registered, active_objects, package_id_dict, \n",
    "                 \\exit_zone_list, status_table_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "        frame - frame to run detection on\n",
    "        reindentify - bool that denotes whether bags have to be reidentified before the next detection iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    global k\n",
    "    \n",
    "    if reindentify:\n",
    "        detections, active_objects, final_box_points = find_bags(frame, False, active_objects, False)\n",
    "            \n",
    "        post_nms_boxes = non_max_suppression_fast(np.array(final_box_points), overlapThresh=0.75)\n",
    "        \n",
    "        remaining_boxes = []\n",
    "        \n",
    "        # delete all boxes that didnt make it past nms\n",
    "        for i in post_nms_boxes:\n",
    "            normal_box = convert_opposite_points_into_normal(i)\n",
    "            if normal_box in detections:\n",
    "                remaining_boxes.append(normal_box)\n",
    "\n",
    "        old_boxes_cnt = len(package_id_dict.keys())\n",
    "        new_boxes_cnt = len(remaining_boxes)\n",
    "        \n",
    "        new_package_id_dict = package_id_dict.copy()\n",
    "        \n",
    "        if new_boxes_cnt == old_boxes_cnt:\n",
    "            for ID, (old_box, frames_missing_since_last_detection, is_constant, status) in package_id_dict.items():\n",
    "                if frames_missing_since_last_detection >= N*(fps/k):\n",
    "                    if not is_constant:\n",
    "                        del new_package_id_dict[ID]\n",
    "                        \n",
    "                        #global objects_deregistered\n",
    "                        objects_deregistered += 1\n",
    "                        active_objects -= 1\n",
    "                        \n",
    "                        #print(f'Object with ID {ID} is deregistered and is no longer being tracked.')\n",
    "                        if status == 'in_exit_zone':\n",
    "                            status_table_dict.update({i: 'delivered'})\n",
    "                        continue    \n",
    "            \n",
    "            new_package_id_dict, _ = reindentify_boxes_v1(remaining_boxes, package_id_dict, new_package_id_dict)\n",
    "            \n",
    "        elif new_boxes_cnt > old_boxes_cnt:\n",
    "            copy_of_remaining_boxes = remaining_boxes.copy()\n",
    "            for ID, (old_box, frames_missing_since_last_detection, is_constant, status) in package_id_dict.items():\n",
    "                if frames_missing_since_last_detection >= N*(fps/k):\n",
    "                    if not is_constant:\n",
    "                        del new_package_id_dict[ID]\n",
    "                        \n",
    "                        objects_deregistered += 1\n",
    "                        active_objects -= 1\n",
    "                        continue\n",
    "                        \n",
    "            new_package_id_dict, unused_new_boxes = reindentify_boxes_v1(remaining_boxes, package_id_dict, new_package_id_dict)\n",
    "            for box_to_add in copy_of_remaining_boxes:\n",
    "                if box_to_add in unused_new_boxes:\n",
    "                    new_ID = objects_registered + 1\n",
    "                    new_package_id_dict.update({new_ID: (box_to_add, 0, False, 'is_preparing')})\n",
    "                    objects_registered += 1\n",
    "                    active_objects += 1\n",
    "        elif new_boxes_cnt < old_boxes_cnt:\n",
    "            used_new_boxes = []\n",
    "            deleted_box_ids = []\n",
    "            for ID, (old_box, frames_missing_since_last_detection, is_constant, status) in package_id_dict.items():\n",
    "                if frames_missing_since_last_detection >= N*(fps/k):\n",
    "                    if not is_constant:\n",
    "                        del new_package_id_dict[ID]\n",
    "                        objects_deregistered += 1\n",
    "                        active_objects -= 1\n",
    "                        deleted_box_ids.append(ID)\n",
    "                        if status == 'in_exit_zone':\n",
    "                            status_table_dict.update({i: 'delivered'})\n",
    "                        continue\n",
    "            new_package_id_dict, non_reidentified_box_ids = reindentify_boxes_v2(remaining_boxes, package_id_dict, new_package_id_dict)\n",
    "            \n",
    "            for i in non_reidentified_box_ids:\n",
    "                if i in deleted_box_ids:\n",
    "                    continue\n",
    "                previous_box_coords = new_package_id_dict[i][0]\n",
    "                previous_frames_missing = new_package_id_dict[i][1]\n",
    "                previous_frames_missing += 6\n",
    "                prev_is_const = new_package_id_dict[i][2]\n",
    "                prev_status = new_package_id_dict[i][3]\n",
    "                \n",
    "                if previous_frames_missing >= N*(fps/k):\n",
    "                    if not is_constant:\n",
    "                        del new_package_id_dict[i]\n",
    "                        objects_deregistered += 1\n",
    "                        active_objects -= 1\n",
    "                        if prev_status == 'in_exit_zone':\n",
    "                            status_table_dict.update({i: 'delivered'})\n",
    "                    continue\n",
    "                if not prev_is_const:   \n",
    "                    new_package_id_dict.update({i: (previous_box_coords, previous_frames_missing, prev_is_const, prev_status)})\n",
    "        return remaining_boxes, new_package_id_dict, objects_deregistered, objects_registered, active_objects, status_table_dict\n",
    "    else:\n",
    "        \n",
    "        detections, active_objects, final_box_points = find_bags(frame, True, active_objects, show=False)\n",
    "        post_nms_boxes = non_max_suppression_fast(np.array(final_box_points), overlapThresh=0.75)\n",
    "        \n",
    "        remaining_boxes = []\n",
    "        \n",
    "        for k, v in package_id_dict.items():\n",
    "            print((k, v))\n",
    "        \n",
    "        # delete all boxes that didnt make it past nms\n",
    "        for i in post_nms_boxes:\n",
    "            normal_box = convert_opposite_points_into_normal(i)\n",
    "            if normal_box in detections:\n",
    "                remaining_boxes.append(normal_box)\n",
    "        \n",
    "        new_package_id_dict = package_id_dict.copy()\n",
    "        \n",
    "        for i, (box, frames_missing, is_constant, status) in package_id_dict.items():\n",
    "            if box not in remaining_boxes:\n",
    "                del new_package_id_dict[i]\n",
    "        \n",
    "        return remaining_boxes, new_package_id_dict, objects_deregistered, objects_registered, active_objects, status_table_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tracker methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tracker(package_id_dict):\n",
    "    \"\"\"\n",
    "        package_id_dict - dict containing IDs and coverted coords of respective bbox\n",
    "        returns - ready-to-update MultiTracker object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create Multitracker object\n",
    "    multi_tracker = cv2.MultiTracker.create()\n",
    "    \n",
    "    # Initialise MultiTracker \n",
    "    for ID, (box, frames_missing, is_constant, status) in package_id_dict.items():\n",
    "        x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "        \n",
    "        # convert frame to greyscale with 1 channel for CSRT tracker\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        preset_frame = np.expand_dims(gray_frame, axis=2)\n",
    "        \n",
    "        if is_constant:\n",
    "            \n",
    "            # init with conf_thresh = 0.07\n",
    "            fp = cv2.FileStorage(\"csrt_psr_0.07.xml\",cv2.FILE_STORAGE_READ)\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "            tracker.read(fp.getFirstTopLevelNode())\n",
    "            multi_tracker.add(tracker, preset_frame, (x, y, w, h))\n",
    "            \n",
    "        elif frames_missing > 0:\n",
    "            # init with conf_thresh = 0.05\n",
    "            fp = cv2.FileStorage(\"csrt_psr_0.05.xml\",cv2.FILE_STORAGE_READ)\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "            tracker.read(fp.getFirstTopLevelNode())\n",
    "            multi_tracker.add(tracker, preset_frame, (x, y, w, h))\n",
    "            \n",
    "        else:\n",
    "            # init with default conf_thresh - 0.035\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "            multi_tracker.add(tracker, preset_frame, (x, y, w, h))\n",
    "        \n",
    "    return multi_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tracker(multi_tracker, frame, status_table_dict, exit_zone_list):\n",
    "    \"\"\"\n",
    "        multi_tracker - initialized tracker to track with\n",
    "        frame - frame to run detection on\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert frame to greyscale with 1 channel for CSRT tracker\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    preset_frame = np.expand_dims(gray_frame, axis=2)\n",
    "    \n",
    "    _, new_boxes = multi_tracker.update(preset_frame)\n",
    "    \n",
    "    new_boxes_list = new_boxes\n",
    "    tracked_frame = frame.copy()\n",
    "    new_package_id_dict = package_id_dict.copy()\n",
    "    \n",
    "    used_old_boxes = []\n",
    "    for new_box in new_boxes_list:\n",
    "        \n",
    "        used_old_boxes = [list(i) for i in used_old_boxes]\n",
    "    \n",
    "        similarity_dict = {}\n",
    "        for i, (ID, (old_box, frames_missing, is_constant, status)) in enumerate(package_id_dict.items()):\n",
    "            old_box = list(old_box)\n",
    "            if old_box in used_old_boxes:\n",
    "                continue\n",
    "            centroid_old = get_centroid(old_box)\n",
    "            centroid_new = get_centroid(new_box)\n",
    "            highly_similar = is_highly_similar(centroid_old, centroid_new)\n",
    "            similarity_dict.update({(ID, frames_missing, is_constant, status): (highly_similar, old_box)})        \n",
    "        \n",
    "        if not similarity_dict:\n",
    "            continue\n",
    "        most_similar_old_box = min(similarity_dict, key=similarity_dict.get)\n",
    "        used_old_boxes.append(similarity_dict[most_similar_old_box][1])\n",
    "        needed_box = similarity_dict[most_similar_old_box]\n",
    "        ID, frames_missing, is_constant, status = most_similar_old_box[0], most_similar_old_box[1], most_similar_old_box[2], most_similar_old_box[3]\n",
    "        new_package_id_dict.update({ID: (new_box, frames_missing, is_constant, status)})\n",
    "        \n",
    "    new_package_id_dict = check_exit_zones(new_package_id_dict, exit_zone_list)\n",
    "    status_table = update_status_table(status_table_dict, new_package_id_dict)\n",
    "\n",
    "    for ID, (new_box, frames_missing, is_constant, status) in new_package_id_dict.items():\n",
    "        if frames_missing > 0:\n",
    "            draw_temporary_boxes(ID, new_box, tracked_frame, status)  \n",
    "        else:\n",
    "            draw_tracked_boxes(ID, new_box, tracked_frame, status)\n",
    "        \n",
    "    draw_status_table(tracked_frame, status_table_dict=status_table)\n",
    "    draw_exit_zones(tracked_frame, exit_zone_list)\n",
    "    \n",
    "    return tracked_frame, new_package_id_dict, new_boxes_list, status_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video processing script (1 detection + 29 trackings / second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional constants\n",
    "FRAMES_PROCESSED = 6\n",
    "VIDEO_DIR = './'\n",
    "VIDEO_SAVE_DIR = 'demo_videos'\n",
    "SECONDS_TO_PROCESS = 20\n",
    "\n",
    "# num of seconds to track the object after its disappearance\n",
    "N = 3\n",
    "USE_RECEIPT_DETECTION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = VIDEO_DIR + video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(full_path)\n",
    "\n",
    "start_idx = 44220 #79940 # \n",
    "\n",
    "h, w, fps, k = get_capture_info(cap)\n",
    "fps = int(fps)\n",
    "k = 1 \n",
    "\n",
    "frame_id = 0\n",
    "\n",
    "cap.release()\n",
    "\n",
    "print(f'Video resolution: {w}x{h}, FPS: {fps}, processing every frame')\n",
    "\n",
    "# create dictionary to store each frame's tracked boxes\n",
    "post_tracking_boxes = {}\n",
    "\n",
    "# create dictionary to store ids and coords of every bbox\n",
    "package_id_dict = {}\n",
    "\n",
    "cycle_started = False\n",
    "cycle_iteration = 0\n",
    "\n",
    "cap = cv2.VideoCapture(full_path)\n",
    "\n",
    "\n",
    "\n",
    "status_table_dict = {}\n",
    "\n",
    "active_objects = 0\n",
    "objects_registered = 0\n",
    "objects_deregistered = 0\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_idx)\n",
    "\n",
    "_, first_frame = cap.read()\n",
    "\n",
    "labeler = ZoneLabeler(first_frame)\n",
    "_, exit_zone_list = labeler.run()\n",
    "\n",
    "seconds_to_process = SECONDS_TO_PROCESS\n",
    "frames_to_process = fps * seconds_to_process\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_idx)\n",
    "    \n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    start_idx += 1\n",
    "    \n",
    "    if not success:\n",
    "        print('Error: issues with the frame')\n",
    "        break\n",
    "        \n",
    "    if not cycle_started: \n",
    "\n",
    "        if not package_id_dict:\n",
    "            detections, package_id_dict, objects_deregistered, objects_registered, active_objects, status_table_dict = run_detector(\n",
    "                frame, reindentify=False, objects_deregistered=objects_deregistered, objects_registered=objects_registered, active_objects=active_objects,\n",
    "                package_id_dict=package_id_dict, exit_zone_list=exit_zone_list, status_table_dict=status_table_dict\n",
    "            )\n",
    "        else:\n",
    "            detections, package_id_dict, objects_deregistered, objects_registered, active_objects, status_table_dict = run_detector(\n",
    "                frame, reindentify=True, objects_deregistered=objects_deregistered, objects_registered=objects_registered, active_objects=active_objects,\n",
    "                package_id_dict=package_id_dict, exit_zone_list=exit_zone_list, status_table_dict=status_table_dict\n",
    "            )\n",
    "            \n",
    "        cycle_started = True\n",
    "        detected_frame = frame.copy()\n",
    "\n",
    "        # render status table\n",
    "        status_table_dict = update_status_table(status_table_dict, package_id_dict)\n",
    "        \n",
    "        # draw initial boxes and their IDs\n",
    "        for ID, (box, frames_missing, is_constant, status) in package_id_dict.items():\n",
    "            \n",
    "            if frames_missing > 0:\n",
    "                draw_temporary_boxes(ID, box, detected_frame, status)  \n",
    "            else:\n",
    "                draw_boxes(ID, box, detected_frame, status)\n",
    "        \n",
    "        draw_status_table(detected_frame, status_table_dict)\n",
    "        draw_exit_zones(detected_frame, exit_zone_list)\n",
    "        \n",
    "        bags_in_img = cut_proper_box(frame, package_id_dict)\n",
    "        \n",
    "        get_receipt_regions(bags_in_img=bags_in_img, use_receipt_detection=USE_RECEIPT_DETECTION)\n",
    "                \n",
    "        name = f'./processed_frames/{frame_id}.jpg'\n",
    "        cv2.imwrite(name, detected_frame)\n",
    "\n",
    "        # init tracker\n",
    "        multi_tracker = init_tracker(package_id_dict)\n",
    "        cycle_iteration += 1\n",
    "        frame_id += 1\n",
    "        print('continuation of loop after detection')\n",
    "        continue\n",
    "\n",
    "    if cycle_iteration == 0 or active_objects == 0:\n",
    "        frame_id += 1\n",
    "        if frame_id > frames_to_process:\n",
    "            break\n",
    "        continue\n",
    "    elif 1 <= cycle_iteration <= 28:\n",
    "        tracked_frame, package_id_dict, new_boxes_list, status_table_dict = run_tracker(multi_tracker, frame, status_table_dict, exit_zone_list)\n",
    "\n",
    "        name = f'./processed_frames/{frame_id}.jpg'\n",
    "        cv2.imwrite(name, tracked_frame)\n",
    "        cycle_iteration += 1\n",
    "\n",
    "    elif cycle_iteration == 29:\n",
    "\n",
    "        tracked_frame, package_id_dict, new_boxes_list, status_table = run_tracker(multi_tracker, frame, status_table_dict, exit_zone_list)\n",
    "        \n",
    "        name = f'./processed_frames/{frame_id}.jpg'\n",
    "        cv2.imwrite(name, tracked_frame)\n",
    "\n",
    "        cycle_started = False\n",
    "        cycle_iteration = 0  \n",
    "    \n",
    "    frame_id += 1\n",
    "    \n",
    "    if frame_id > frames_to_process:\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(outvid, images=None, fps=30, size=None,\n",
    "               is_color=True, format=\"FMP4\"):\n",
    "    \"\"\"\n",
    "    Create a video from a list of images.\n",
    " \n",
    "    @param      outvid      output video\n",
    "    @param      images      list of images to use in the video\n",
    "    @param      fps         frame per second\n",
    "    @param      size        size of each frame\n",
    "    @param      is_color    color\n",
    "    @param      format      see http://www.fourcc.org/codecs.php\n",
    "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    " \n",
    "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
    "    By default, the video will have the size of the first image.\n",
    "    It will resize every image to this size before adding them to the video.\n",
    "    \"\"\"\n",
    "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
    "    fourcc = VideoWriter_fourcc(*format)\n",
    "    vid = None\n",
    "    for image in images:\n",
    "        if not os.path.exists(image):\n",
    "            raise FileNotFoundError(image)\n",
    "        img = imread(image)\n",
    "        if vid is None:\n",
    "            if size is None:\n",
    "                size = img.shape[1], img.shape[0]\n",
    "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
    "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
    "            img = resize(img, size)\n",
    "        vid.write(img)\n",
    "    vid.release()\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of images to run detection on\n",
    "ROOT_DIR = os.getcwd()\n",
    "VIDEO_SAVE_DIR = os.path.join(ROOT_DIR, \"demo_videos\")\n",
    "IMAGES_DIR = os.path.join(ROOT_DIR, \"processed_frames\")\n",
    "images = list(glob.iglob(os.path.join(IMAGES_DIR, '*.*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "outvid = os.path.join(VIDEO_SAVE_DIR, \"30_fps_video_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<VideoWriter 00000212B26A4DD0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_video(outvid, images, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
