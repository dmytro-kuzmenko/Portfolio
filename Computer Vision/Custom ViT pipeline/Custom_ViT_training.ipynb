{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa2e29f",
   "metadata": {},
   "source": [
    "## Custom ViT adaptation\n",
    "* @author Dmytro Kuzmenko\n",
    "* fully customizable train pipeline for CV tasks\n",
    "* dataset used in the example: tiny-imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!! wget http://cs231n.stanford.edu/tiny-imagenet-200.zip -O ../data/tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "72735991",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from dataclasses import dataclass, replace\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Sequence, Union, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import importlib\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.dataset import Dataset\n",
    "#import sklearn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.nn.modules import loss\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "413ad908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.9.0+cu102\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "#print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930e5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f9bc2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd062777",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform = Callable[[Image.Image], Image.Image]\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class ItemsBatch:\n",
    "    images: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    ids: List[int]\n",
    "    paths: List[Path]\n",
    "    items: List[\"DatasetItem\"]\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class DatasetItem:\n",
    "    image: Union[torch.Tensor, Image.Image]\n",
    "    label: int\n",
    "    id: int\n",
    "    path: Path\n",
    "\n",
    "    @classmethod\n",
    "    def collate(cls, items: Sequence[\"DatasetItem\"]) -> ItemsBatch:\n",
    "        if not isinstance(items, list):\n",
    "            items = list(items)\n",
    "        return ItemsBatch(\n",
    "            images=default_collate([item.image for item in items]),\n",
    "            labels=default_collate([item.label for item in items]),\n",
    "            ids=[item.id for item in items],\n",
    "            paths=[item.path for item in items],\n",
    "            items=items,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f199da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\tmodelling_pipeline_v1.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcabf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '../data/hopefuly_final/tiny-imagenet-200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b07c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = DATA_ROOT + \"train\"\n",
    "VAL_PATH = DATA_ROOT + \"val\"\n",
    "ALL_FOLDERS = [\n",
    "    dir_name\n",
    "    for r, d, f in os.walk(TRAIN_PATH)\n",
    "    for dir_name in d\n",
    "    if dir_name != \"images\"\n",
    "]\n",
    "FOLDERS_TO_NUM = {val: index for index, val in enumerate(ALL_FOLDERS)}\n",
    "\n",
    "LABELS = pd.read_csv(\n",
    "    DATA_ROOT + \"words.txt\", sep=\"\\t\", header=None, index_col=0)[1].to_dict()\n",
    "VAL_LABELS = pd.read_csv(\n",
    "    DATA_ROOT + \"val/\" + \"val_annotations.txt\", sep=\"\\t\", header=None, index_col=0)[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "292d5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImagenetDataset(Dataset):\n",
    "    _transform: Optional[Transform]\n",
    "    _root: Path\n",
    "    _df: DataFrame\n",
    "\n",
    "    def __init__(self, path, transform: Optional[Transform] = None):\n",
    "        self._transform = transform\n",
    "        if not os.path.isdir(path):\n",
    "            raise NotADirectoryError(f\"{path} is not a directory.\")\n",
    "        all_files = [\n",
    "            os.path.join(r, fyle)\n",
    "            for r, d, f in os.walk(path)\n",
    "            for fyle in f\n",
    "            if \".JPEG\" in fyle\n",
    "        ]\n",
    "        labels = [\n",
    "            FOLDERS_TO_NUM.get(\n",
    "                os.path.basename(f).split(\"_\")[0],\n",
    "                FOLDERS_TO_NUM.get(VAL_LABELS.get(os.path.basename(f))),\n",
    "            )\n",
    "            for f in all_files\n",
    "        ]\n",
    "        self._df = pd.DataFrame({\"path\": all_files, \"label\": labels})\n",
    "\n",
    "    def __getitem__(self, index: int) -> DatasetItem:\n",
    "        path, label = self._df.loc[index, :]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        if self._transform:\n",
    "            image = self._transform(image)\n",
    "        return DatasetItem(image=image, label=label, id=index, path=path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e41e0",
   "metadata": {},
   "source": [
    "## Modules setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc9be8",
   "metadata": {},
   "source": [
    "### Custom ViT using timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "79439f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e71bd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b50afd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7802e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "80d49701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2a12759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9505b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=64, patch_size=8, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c9a94ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n",
    "        - https://arxiv.org/abs/2012.12877\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=64, patch_size=8, in_chans=3, num_classes=200, embed_dim=192, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None, weight_init=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            weight_init: (str): weight init scheme\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.dist_token is not None:\n",
    "            trunc_normal_(self.dist_token, std=.02)\n",
    "        if mode.startswith('jax'):\n",
    "            # leave cls token as zeros to match jax impl\n",
    "            named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)\n",
    "        else:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "            self.apply(_init_vit_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        _init_vit_weights(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        if self.dist_token is None:\n",
    "            return self.head\n",
    "        else:\n",
    "            return self.head, self.head_dist\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        if self.num_tokens == 2:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):\n",
    "    \"\"\" ViT weight initialization\n",
    "    * When called without n, head_bias, jax_impl args it will behave exactly the same\n",
    "      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n",
    "    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if name.startswith('head'):\n",
    "            nn.init.zeros_(module.weight)\n",
    "            nn.init.constant_(module.bias, head_bias)\n",
    "        elif name.startswith('pre_logits'):\n",
    "            lecun_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            if jax_impl:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    if 'mlp' in name:\n",
    "                        nn.init.normal_(module.bias, std=1e-6)\n",
    "                    else:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "            else:\n",
    "                trunc_normal_(module.weight, std=.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    elif jax_impl and isinstance(module, nn.Conv2d):\n",
    "        # NOTE conv was left to pytorch default in my original init\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n",
    "    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def _n2p(w, t=True):\n",
    "        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n",
    "            w = w.flatten()\n",
    "        if t:\n",
    "            if w.ndim == 4:\n",
    "                w = w.transpose([3, 2, 0, 1])\n",
    "            elif w.ndim == 3:\n",
    "                w = w.transpose([2, 0, 1])\n",
    "            elif w.ndim == 2:\n",
    "                w = w.transpose([1, 0])\n",
    "        return torch.from_numpy(w)\n",
    "\n",
    "    w = np.load(checkpoint_path)\n",
    "    if not prefix and 'opt/target/embedding/kernel' in w:\n",
    "        prefix = 'opt/target/'\n",
    "\n",
    "    if hasattr(model.patch_embed, 'backbone'):\n",
    "        # hybrid\n",
    "        backbone = model.patch_embed.backbone\n",
    "        stem_only = not hasattr(backbone, 'stem')\n",
    "        stem = backbone if stem_only else backbone.stem\n",
    "        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n",
    "        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n",
    "        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n",
    "        if not stem_only:\n",
    "            for i, stage in enumerate(backbone.stages):\n",
    "                for j, block in enumerate(stage.blocks):\n",
    "                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n",
    "                    for r in range(3):\n",
    "                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n",
    "                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n",
    "                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n",
    "                    if block.downsample is not None:\n",
    "                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n",
    "                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n",
    "                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n",
    "        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n",
    "    else:\n",
    "        embed_conv_w = adapt_input_conv(\n",
    "            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n",
    "    model.patch_embed.proj.weight.copy_(embed_conv_w)\n",
    "    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n",
    "    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n",
    "    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n",
    "    if pos_embed_w.shape != model.pos_embed.shape:\n",
    "        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights\n",
    "            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "    model.pos_embed.copy_(pos_embed_w)\n",
    "    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n",
    "    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n",
    "    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n",
    "        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n",
    "        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n",
    "    if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n",
    "        model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n",
    "        model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n",
    "    for i, block in enumerate(model.blocks.children()):\n",
    "        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n",
    "        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'\n",
    "        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        block.attn.qkv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n",
    "        block.attn.qkv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n",
    "        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))\n",
    "            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))\n",
    "        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))\n",
    "        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=()):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if num_tokens:\n",
    "        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n",
    "        ntok_new -= num_tokens\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    if not len(gs_new):  # backwards compatibility\n",
    "        gs_new = [int(math.sqrt(ntok_new))] * 2\n",
    "    assert len(gs_new) >= 2\n",
    "    _logger.info('Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bilinear')\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if 'model' in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict['model']\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "            # For old models that I trained prior to conv based patchification\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            v = v.reshape(O, -1, H, W)\n",
    "        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(\n",
    "                v, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "        out_dict[k] = v\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5efcd129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06a686b6",
   "metadata": {},
   "source": [
    "### Tensorboard & metrics tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27951280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardWriter():\n",
    "    def __init__(self, log_dir, enabled):\n",
    "        self.writer = None\n",
    "        self.selected_module = \"\"\n",
    "\n",
    "        if enabled:\n",
    "            log_dir = str(log_dir)\n",
    "\n",
    "            # Retrieve vizualization writer.\n",
    "            succeeded = False\n",
    "            for module in [\"torch.utils.tensorboard\", \"tensorboardX\"]:\n",
    "                try:\n",
    "                    self.writer = importlib.import_module(module).SummaryWriter(log_dir)\n",
    "                    succeeded = True\n",
    "                    break\n",
    "                except ImportError:\n",
    "                    succeeded = False\n",
    "                self.selected_module = module\n",
    "\n",
    "            if not succeeded:\n",
    "                message = \"Warning: visualization (Tensorboard) is configured to use, but currently not installed on \" \\\n",
    "                    \"this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to \" \\\n",
    "                    \"version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\"\n",
    "                print(message)\n",
    "\n",
    "        self.step = 0\n",
    "        self.mode = ''\n",
    "\n",
    "        self.tb_writer_ftns = {\n",
    "            'add_scalar', 'add_scalars', 'add_image', 'add_images', 'add_audio',\n",
    "            'add_text', 'add_histogram', 'add_pr_curve', 'add_embedding'\n",
    "        }\n",
    "        self.tag_mode_exceptions = {'add_histogram', 'add_embedding'}\n",
    "        self.timer = datetime.now()\n",
    "\n",
    "    def set_step(self, step, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.step = step\n",
    "        if step == 0:\n",
    "            self.timer = datetime.now()\n",
    "        else:\n",
    "            duration = datetime.now() - self.timer\n",
    "            self.add_scalar('steps_per_sec', 1 / duration.total_seconds())\n",
    "            self.timer = datetime.now()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        If visualization is configured to use:\n",
    "            return add_data() methods of tensorboard with additional information (step, tag) added.\n",
    "        Otherwise:\n",
    "            return a blank function handle that does nothing\n",
    "        \"\"\"\n",
    "        if name in self.tb_writer_ftns:\n",
    "            add_data = getattr(self.writer, name, None)\n",
    "\n",
    "            def wrapper(tag, data, *args, **kwargs):\n",
    "                if add_data is not None:\n",
    "                    # add mode(train/valid) tag\n",
    "                    if name not in self.tag_mode_exceptions:\n",
    "                        tag = '{}/{}'.format(tag, self.mode)\n",
    "                    if name == 'add_embedding':\n",
    "                        add_data(tag=tag, mat=data, global_step=self.step, *args, **kwargs)\n",
    "                    else:\n",
    "                        add_data(tag, data, self.step, *args, **kwargs)\n",
    "            return wrapper\n",
    "        else:\n",
    "            \n",
    "            # default action for returning methods defined in this class, set_step() for instance.\n",
    "            try:\n",
    "                attr = object.__getattr__(name)\n",
    "            except AttributeError:\n",
    "                raise AttributeError(\"type object '{}' has no attribute '{}'\".format(self.selected_module, name))\n",
    "            return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e410f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricTracker:\n",
    "    def __init__(self, *keys, writer=None):\n",
    "        self.writer = writer\n",
    "        self._data = pd.DataFrame(index=keys, columns=['total', 'counts', 'average'])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for col in self._data.columns:\n",
    "            self._data[col].values[:] = 0\n",
    "\n",
    "    def update(self, key, value, n=1):\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar(key, value)\n",
    "        self._data.total[key] += value * n\n",
    "        self._data.counts[key] += n\n",
    "        self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
    "\n",
    "    def avg(self, key):\n",
    "        return self._data.average[key]\n",
    "\n",
    "    def result(self):\n",
    "        return dict(self._data.average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e654362",
   "metadata": {},
   "source": [
    "### custom accuracy for top 1 and top k(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fa065da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k / batch_size * 100.0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e7ce6",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1d981dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/hopefuly_final/tiny-imagenet-200/'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "12c82f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        # add flips, rotation etc\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28b4d1c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(DATA_ROOT + 'train', transform_train)\n",
    "val_dataset = datasets.ImageFolder(DATA_ROOT + 'val', transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "39a17956",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a1798",
   "metadata": {},
   "source": [
    "## Train/val methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0edfef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, data_loader, criterion, optimizer, lr_scheduler, metrics, device=torch.device('cpu')):\n",
    "    metrics.reset()\n",
    "\n",
    "    # training loop\n",
    "    for batch_idx, (batch_data, batch_target) in enumerate(data_loader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_pred = model(batch_data)\n",
    "        loss = criterion(batch_pred, batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        acc1, acc5 = accuracy(batch_pred, batch_target, topk=(1, 5))\n",
    "\n",
    "        metrics.writer.set_step((epoch - 1) * len(data_loader) + batch_idx)\n",
    "        metrics.update('loss', loss.item())\n",
    "        metrics.update('acc1', acc1.item())\n",
    "        metrics.update('acc5', acc5.item())\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Train Epoch: {:03d} Batch: {:05d}/{:05d} Loss: {:.4f} Acc@1: {:.2f}, Acc@5: {:.2f}\"\n",
    "                    .format(epoch, batch_idx, len(data_loader), loss.item(), acc1.item(), acc5.item()))\n",
    "    return metrics.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "067018f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(epoch, model, data_loader, criterion, metrics, device=torch.device('cpu')):\n",
    "    metrics.reset()\n",
    "    losses = []\n",
    "    acc1s = []\n",
    "    acc5s = []\n",
    "    \n",
    "    # validation loop\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch_data, batch_target) in enumerate(data_loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "\n",
    "            batch_pred = model(batch_data)\n",
    "            loss = criterion(batch_pred, batch_target)\n",
    "            acc1, acc5 = accuracy(batch_pred, batch_target, topk=(1, 5))\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            acc1s.append(acc1.item())\n",
    "            acc5s.append(acc5.item())\n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    acc1 = np.mean(acc1s)\n",
    "    acc5 = np.mean(acc5s)\n",
    "    metrics.writer.set_step(epoch, 'valid')\n",
    "    metrics.update('loss', loss)\n",
    "    metrics.update('acc1', acc1)\n",
    "    metrics.update('acc5', acc5)\n",
    "    \n",
    "    return metrics.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a59119df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(save_dir, epoch, model, optimizer, lr_scheduler, best=False):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "    }\n",
    "    filename = str(save_dir + 'current.pth')\n",
    "    torch.save(state, filename)\n",
    "\n",
    "    if best:\n",
    "        filename = str(save_dir + 'best.pth')\n",
    "        torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e4639",
   "metadata": {},
   "source": [
    "## Training prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c140f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dir = '../results/summary/'\n",
    "checkpoint_dir = '../results/checkpoint/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ee89d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "writer = TensorboardWriter(summary_dir, enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "494edadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric tracker\n",
    "metric_names = ['loss', 'acc1', 'acc5']\n",
    "\n",
    "train_metrics = MetricTracker(*[metric for metric in metric_names], writer=writer)\n",
    "valid_metrics = MetricTracker(*[metric for metric in metric_names], writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c5d0e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = VisionTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0b95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "de3216ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU.\n"
     ]
    }
   ],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu == 0:\n",
    "    print(\"Running on CPU.\")\n",
    "    n_gpu_use = 0\n",
    "else:\n",
    "    print(\"Running on the GPU.\")\n",
    "device = torch.device('cuda:0' if n_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "266f7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "68c10eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "888110b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training criterion\n",
    "# nllloss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "14f4a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizers and learning rate scheduler\n",
    "# experiment\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=0.03,\n",
    "    weight_decay=0.0001,\n",
    "    momentum=0.9)\n",
    "\n",
    "# experiment\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    max_lr=0.03,\n",
    "    pct_start=500 / 10000,\n",
    "    total_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "eaebb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "epochs = 10000 // len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55175116",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c78dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    log = {'epoch': epoch}\n",
    "\n",
    "    # train the model\n",
    "    model.train()\n",
    "    result = train_epoch(epoch, model, train_loader, criterion, optimizer, lr_scheduler, train_metrics, device)\n",
    "    log.update(result)\n",
    "\n",
    "    # validate the model\n",
    "    model.eval()\n",
    "    result = valid_epoch(epoch, model, val_loader, criterion, valid_metrics, device)\n",
    "    log.update(**{'val_' + k: v for k, v in result.items()})\n",
    "\n",
    "    # best acc\n",
    "    best = False\n",
    "    if log['val_acc1'] > best_acc:\n",
    "        best_acc = log['val_acc1']\n",
    "        best = True\n",
    "\n",
    "    # save model\n",
    "    save_model(checkpoint_dir, epoch, model, optimizer, lr_scheduler, best)\n",
    "\n",
    "    # print logged informations to the screen\n",
    "    for key, value in log.items():\n",
    "        print('    {:15s}: {}'.format(str(key), value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfce80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7891d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
